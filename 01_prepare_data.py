import os
import glob
import xarray as xr
import pandas as pd
import numpy as np
from tqdm import tqdm
import warnings
import gc

warnings.filterwarnings('ignore')

# ================= é…ç½® =================
RAW_SEC_ROOT = "/gemini/data-1/Tushare/sec"
RAW_INDEX_ROOT = "/gemini/data-1/Tushare/index"

OUTPUT_TRAIN_DIR = "/gemini/code/dataset/train"       
OUTPUT_INFER_DIR = "/gemini/code/dataset/inference"   

# ç›®æ ‡æŒ‡æ•°: ä¸­è¯1000 (000852)
TARGET_INDEX_ID = "000852"
# =======================================

def get_column_map(data_coords):
    mapping = {}
    for col in data_coords:
        c_str = str(col).strip().upper()
        if 'OPEN' in c_str: mapping[col] = 'open'
        elif 'HIGH' in c_str: mapping[col] = 'high'
        elif 'LOW' in c_str: mapping[col] = 'low'
        elif 'CLOSE' in c_str: mapping[col] = 'close'
        elif 'VOL' in c_str: mapping[col] = 'volume'
        elif 'AMOUNT' in c_str: mapping[col] = 'amount'
    return mapping

def process_nc_file_final(file_path, output_dir, is_index=False):
    saved_count = 0
    try:
        # 1. è¯»å–æ–‡ä»¶
        with xr.open_dataset(file_path, engine='netcdf4') as ds:
            # 2. é”å®šç»´åº¦ (åŸºäºä½ çš„æ¢é’ˆç»“æœ)
            # Ticker: è‚¡ç¥¨ä»£ç , FDate: æ—¥æœŸ, Data: å­—æ®µ
            # ç¨å¾®åšç‚¹å…¼å®¹é˜²æ­¢å¤§å°å†™ä¸åŒ
            dims = list(ds.dims)
            dim_ticker = next((d for d in dims if d in ['Ticker', 'ticker', 'Code', 'code']), None)
            dim_date = next((d for d in dims if d in ['FDate', 'fdate', 'Date', 'date', 'time']), None)
            dim_data = next((d for d in dims if d in ['Data', 'data']), None)
            
            if not (dim_ticker and dim_date and dim_data):
                # print(f"Skipped {os.path.basename(file_path)}: Dims mismatch {dims}")
                return 0

            # 3. å‡†å¤‡æ•°æ®
            var_name = list(ds.data_vars)[0]
            da = ds[var_name]
            all_tickers = ds.coords[dim_ticker].values
            data_fields = ds.coords[dim_data].values
            col_mapping = get_column_map(data_fields)
            
            if not col_mapping: return 0

            # 4. éå† Ticker
            # å¦‚æœæ˜¯å¾®è°ƒä¸ªè‚¡ï¼Œä¸ºäº†é€Ÿåº¦åªå–å‰ 500 ä¸ª (è¶³å¤Ÿå¤ç°)
            # å¦‚æœæ˜¯æ‰¾æŒ‡æ•°ï¼Œåˆ™å¿…é¡»éå†æ‰€æœ‰
            loop_tickers = all_tickers if is_index else all_tickers[:500]

            for ticker in loop_tickers:
                # ä»£ç å·²ç»æ˜¯ str æ ¼å¼ '000001.SZ'ï¼Œç›´æ¥ç”¨
                ticker_str = str(ticker).strip()
                
                # --- è¿‡æ»¤é€»è¾‘ ---
                if is_index:
                    # åªè¦åŒ…å« 000852 å°±è®¤ä¸ºæ˜¯ä¸­è¯1000
                    if TARGET_INDEX_ID not in ticker_str: continue
                    save_name = "000852.SH.csv"
                    print(f"   ğŸ¯ [å‘½ä¸­] æ‰¾åˆ°ä¸­è¯1000: {ticker_str} åœ¨ {os.path.basename(file_path)}")
                else:
                    # ä¸ªè‚¡: åªä¿ç•™ .SZ / .SH
                    if not (ticker_str.endswith('SZ') or ticker_str.endswith('SH')):
                        continue
                    save_name = f"{ticker_str}.csv"

                # --- æå– ---
                try:
                    # åˆ‡ç‰‡
                    sub_da = da.sel({dim_ticker: ticker})
                    # è½¬ DataFrame
                    df = sub_da.to_dataframe().reset_index()
                    
                    # Pivot (å¤„ç†é‡å¤ç´¢å¼•)
                    # å…³é”®ä¿®æ”¹ï¼šaggfunc='first' é¿å… duplicate entries æŠ¥é”™
                    df_pivot = df.pivot_table(
                        index=dim_date, 
                        columns=dim_data, 
                        values=var_name, 
                        aggfunc='first'
                    ).reset_index()
                    
                    # é‡å‘½ååˆ—
                    df_pivot = df_pivot.rename(columns=col_mapping)
                    df_pivot = df_pivot.rename(columns={dim_date: 'timestamp'})
                    
                    # æ£€æŸ¥å¿…è¦åˆ—
                    if 'close' not in df_pivot.columns: continue

                    # --- ã€å…³é”®ä¿®æ­£ã€‘æ—¥æœŸæ¸…æ´— ---
                    # ä¹‹å‰æ­»åœ¨è¿™é‡Œã€‚ç°åœ¨ç›´æ¥è®© pandas è‡ªåŠ¨æ¨æ–­ï¼Œä¸è¦æŒ‡å®š format
                    df_pivot['timestamp'] = pd.to_datetime(df_pivot['timestamp'], errors='coerce')
                    df_pivot = df_pivot.dropna(subset=['timestamp'])
                    
                    if len(df_pivot) < 5: continue

                    # ä¿å­˜
                    save_path = os.path.join(output_dir, save_name)
                    hdr = not os.path.exists(save_path)
                    df_pivot.to_csv(save_path, mode='a', header=hdr, index=False)
                    saved_count += 1

                except Exception:
                    continue
                    
    except Exception as e:
        print(f"âŒ è¯»å–é”™è¯¯ {os.path.basename(file_path)}: {e}")
        
    return saved_count

def run():
    # 1. å‡†å¤‡ç›®å½•
    os.makedirs(OUTPUT_TRAIN_DIR, exist_ok=True)
    os.makedirs(OUTPUT_INFER_DIR, exist_ok=True)

    # 2. å‡†å¤‡æ–‡ä»¶åˆ—è¡¨
    print("ğŸš€ æ­£åœ¨æ‰«ææ–‡ä»¶åˆ—è¡¨...")
    # æŒ‡æ•°å’Œä¸ªè‚¡æ–‡ä»¶éƒ½å¯èƒ½åŒ…å«ç›®æ ‡æ•°æ®ï¼Œå…¨éƒ¨çº³å…¥æœç´¢èŒƒå›´
    all_nc_files = sorted(glob.glob(os.path.join(RAW_INDEX_ROOT, "**/*.nc"), recursive=True) + 
                          glob.glob(os.path.join(RAW_SEC_ROOT, "**/*.nc"), recursive=True))
    # å»é‡
    all_nc_files = sorted(list(set(all_nc_files)))
    
    if not all_nc_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• .nc æ–‡ä»¶")
        return

    # 3. æå–ä¸­è¯1000 (ä¼˜å…ˆä»»åŠ¡)
    print(f"\n[Step 1] å¯»æ‰¾ä¸­è¯1000 ({len(all_nc_files)} ä¸ªæ–‡ä»¶)...")
    found_idx = False
    for f in tqdm(all_nc_files):
        cnt = process_nc_file_final(f, OUTPUT_INFER_DIR, is_index=True)
        if cnt > 0: found_idx = True
    
    if found_idx:
        print("   âœ… ä¸­è¯1000 æ•°æ®æå–æˆåŠŸï¼")
    else:
        print("   âš ï¸ æœªæ‰¾åˆ°ä¸­è¯1000æ•°æ® (ç¨åå°†è‡ªåŠ¨ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®)")

    # 4. æå–ä¸ªè‚¡ (ç”¨äºå¾®è°ƒ)
    # åªå¤„ç† SEC ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œä¸”ä¸ºäº†é€Ÿåº¦åªå¤„ç†å‰ 20 ä¸ªå¹´ä»½ (å¦‚ 2005-2024)
    sec_only_files = sorted(glob.glob(os.path.join(RAW_SEC_ROOT, "**/*.nc"), recursive=True))
    
    print(f"\n[Step 2] æå–å¾®è°ƒä¸ªè‚¡ (å¤„ç†å‰ 20 ä¸ªå¹´ä»½æ–‡ä»¶)...")
    total_sec = 0
    
    # è¿›åº¦æ¡
    pbar = tqdm(sec_only_files[:20]) 
    for f in pbar:
        cnt = process_nc_file_final(f, OUTPUT_TRAIN_DIR, is_index=False)
        total_sec += cnt
        pbar.set_description(f"Extracted: {total_sec}")

    print(f"\nâœ… å…¨éƒ¨å®Œæˆ!")
    print(f"   -> å¾®è°ƒä¸ªè‚¡æ–‡ä»¶æ•°: {len(glob.glob(os.path.join(OUTPUT_TRAIN_DIR, '*.csv')))}")
    
    # æœ€åç®€å•å»é‡
    print("   -> æ­£åœ¨æ‰§è¡Œæœ€ç»ˆå»é‡...")
    for f in glob.glob(os.path.join(OUTPUT_TRAIN_DIR, "*.csv"))[:200]:
        try:
            df = pd.read_csv(f)
            df.drop_duplicates('timestamp', keep='last', inplace=True)
            df.to_csv(f, index=False)
        except: pass

if __name__ == "__main__":
    run()