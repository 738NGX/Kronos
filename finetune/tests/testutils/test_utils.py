"""
é€šç”¨æµ‹è¯•å·¥å…·å‡½æ•°
"""
import os
import sys
import random
import pandas as pd
import numpy as np
import torch
import torch.distributed as dist
import matplotlib.pyplot as plt
import argparse
import warnings

def init_distributed_mode():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼æ¨ç†/è®­ç»ƒç¯å¢ƒ"""
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ.get("LOCAL_RANK", 0))

        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl", init_method="env://")
        print(f"ğŸ”¥ [DDP] è¿›ç¨‹å¯åŠ¨: Global Rank {rank} | Local Rank {local_rank} | Total {world_size}")
        return rank, local_rank, world_size

    print("âš ï¸ [Single] å•å¡æ¨¡å¼è¿è¡Œ")
    return 0, 0, 1

def setup_environment(seed: int = 100):
    """
    é…ç½®æµ‹è¯•ç¯å¢ƒï¼ˆå­—ä½“ã€è·¯å¾„ã€éšæœºæ•°ç§å­ï¼‰
    
    Args:
        seed: int, éšæœºæ•°ç§å­ï¼ˆé»˜è®¤100ï¼Œä¸config.pyä¿æŒä¸€è‡´ï¼‰
    """
    # ================= éšæœºæ•°ç§å­è®¾ç½® =================
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # ================= ç¯å¢ƒé…ç½® =================
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.family'] = 'Noto Serif CJK JP'
    plt.rcParams['axes.unicode_minus'] = False
    
    # ç¡®ä¿æ¨¡å‹è·¯å¾„å¯è®¿é—®
    sys.path.append("/gemini/code/")
    
    # å¿½ç•¥è­¦å‘Š
    warnings.filterwarnings('ignore')

def save_prediction_results(predictions, output_dir, model_name, index_name):
    """
    ä¿å­˜å•ä¸ªæŒ‡æ•°çš„é¢„æµ‹ç»“æœ
    
    Args:
        predictions: list, é¢„æµ‹ç»“æœåˆ—è¡¨
        output_dir: str, è¾“å‡ºç›®å½•
        model_name: str, æ¨¡å‹åç§° (base/finetuned)
        index_name: str, æŒ‡æ•°åç§°
    
    Returns:
        pd.DataFrame: é¢„æµ‹ç»“æœDataFrame
    """
    res_df = pd.DataFrame(predictions)
    output_path = os.path.join(output_dir, f"predictions_{model_name}_{index_name}.csv")
    res_df.to_csv(output_path, index=False)
    return res_df


def aggregate_and_save_metrics(all_results, output_dir, model_name):
    """
    åŸºäºå…¨æ ·æœ¬æ‹¼æ¥åçš„å®Œæ•´æ—¶é—´åºåˆ—é‡æ–°è®¡ç®—æŒ‡æ ‡å¹¶ä¿å­˜
    
    Args:
        all_results: dict æˆ– DataFrameï¼Œæ‰€æœ‰æŒ‡æ•°çš„å®Œæ•´é¢„æµ‹ç»“æœ
        output_dir: str, è¾“å‡ºç›®å½•
        model_name: str, æ¨¡å‹åç§°
    
    Returns:
        pd.DataFrame: æ±‡æ€»åçš„æŒ‡æ ‡DataFrame
    """
    from testutils.metrics_utils import calculate_metrics, save_and_print_metrics
    
    # å¤„ç† DataFrame è¾“å…¥ï¼ˆç›´æ¥è°ƒç”¨æ—¶ï¼‰
    if isinstance(all_results, pd.DataFrame):
        if all_results.empty:
            return None
        save_and_print_metrics(all_results, output_dir, model_name=model_name)
        return all_results
    
    # å¤„ç†å­—å…¸è¾“å…¥
    if not all_results:
        return None
    
    all_metrics = []
    
    # å¯¹æ¯ä¸ªæŒ‡æ•°çš„å®Œæ•´æ—¶é—´åºåˆ—è®¡ç®—æŒ‡æ ‡
    for name, full_df in all_results.items():
        if full_df is None or full_df.empty:
            continue
        
        # åŸºäºå®Œæ•´æ—¶é—´åºåˆ—è®¡ç®—æŒ‡æ ‡ï¼ˆåŒ…å«è·¨æ—¶é—´æ®µçš„è¶‹åŠ¿ï¼‰
        idx_metrics = calculate_metrics(full_df)
        idx_metrics["Index"] = name
        all_metrics.append(idx_metrics)
    
    if all_metrics:
        final_df = pd.concat(all_metrics, ignore_index=True)
        save_and_print_metrics(final_df, output_dir, model_name=model_name)
        return final_df
    return None


def plot_all_results(all_results, output_dir, model_name, test_config, combine_subplots=True):
    """
    ç»˜åˆ¶æ‰€æœ‰æŒ‡æ•°çš„é¢„æµ‹æ›²çº¿
    
    Args:
        all_results: dict, æ‰€æœ‰æŒ‡æ•°çš„é¢„æµ‹ç»“æœ
        output_dir: str, è¾“å‡ºç›®å½•
        model_name: str, æ¨¡å‹åç§°
        test_config: dict, æµ‹è¯•é…ç½®
        combine_subplots: bool, æ˜¯å¦ç»„åˆä¸ºå¤§å›¾
    """
    from testutils.visualization_utils import plot_predictions
    
    if all_results:
        print("\nğŸ¨ å¼€å§‹ç»˜åˆ¶é¢„æµ‹æ›²çº¿...")
        plot_predictions(all_results, output_dir, model_name=model_name, 
                        test_config=test_config, combine_subplots=combine_subplots)


def parse_test_args(description):
    """
    è§£ææµ‹è¯•è„šæœ¬çš„å‘½ä»¤è¡Œå‚æ•°
    
    Args:
        description: str, è„šæœ¬æè¿°
    
    Returns:
        argparse.Namespace: è§£æåçš„å‚æ•°
    """
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('--separate-plots', action='store_true', 
                       help='è¾“å‡ºç‹¬ç«‹å›¾è¡¨è€Œéç»„åˆå¤§å›¾')
    return parser.parse_args()


def run_distributed_inference(
    predictor,
    all_data,
    indices_dict,
    config,
    output_dir,
    model_name,
    rank: int = 0,
    world_size: int = 1,
):
    from testutils.data_utils import load_and_prepare_index_data

    if rank == 0:
        print("ğŸ“¥ åŠ è½½æ‰€æœ‰æŒ‡æ•°æ•°æ®...")

    indices_data = {}
    test_indices_dict = {}

    for name, symbol in indices_dict.items():
        df, test_indices = load_and_prepare_index_data(all_data, name, symbol, config)
        indices_data[name] = df
        test_indices_dict[name] = test_indices

    # åŸºäºæ—¥æœŸè¿›è¡ŒåŒ¹é…
    test_dates_dict = {}
    for name in indices_data.keys():
        df = indices_data[name]
        valid_dates = [df.iloc[iloc_idx]["date"] for iloc_idx in test_indices_dict[name] if iloc_idx >= config['lookback']]
        test_dates_dict[name] = set(valid_dates)
    
    all_dates_set = [test_dates_dict[name] for name in indices_data.keys()]
    common_dates = sorted(list(set.intersection(*all_dates_set)))

    # ä¸ºæ¯ä¸ªæŒ‡æ•°å»ºç«‹æ—¥æœŸåˆ°è¡Œä½ç½®ç´¢å¼•çš„æ˜ å°„
    date_to_idx_map = {
        name: {date: iloc_idx for iloc_idx, date in enumerate(df["date"])}
        for name, df in indices_data.items()
    }

    my_dates = common_dates[rank::world_size]

    if rank == 0:
        print(f"ğŸ”„ åˆ†å¸ƒå¼æ¨ç†: {len(indices_data)} ä¸ªæŒ‡æ•° Ã— {len(common_dates)} å¤©")
        print(f"   âš™ï¸ æ˜¾å¡æ•°: {world_size} | å•å¡ä»»åŠ¡: ~{len(my_dates)}")

    local_results = {name: [] for name in indices_data.keys()}

    import time
    start_time = time.time()
    log_interval = max(1, len(my_dates) // 5)

    for i, current_date in enumerate(my_dates):
        batch_inputs = []
        batch_x_timestamps = []
        batch_y_timestamps = []
        batch_names = []
        batch_current_closes = []
        batch_current_dates = []
        batch_future_dfs = []

        for name in indices_data.keys():
            df = indices_data[name]
            idx = date_to_idx_map[name].get(current_date)
            
            # ç»“æ„æ€§è¿‡æ»¤é€»è¾‘
            if idx is None or idx < config['lookback'] or idx + config['pred_len'] >= len(df):
                continue
            
            input_df = df.iloc[idx - config['lookback'] : idx].copy()
            current_close = df.iloc[idx]["close"]
            future_df = df.iloc[idx + 1 : idx + 1 + config['pred_len']].copy().reset_index(drop=True)
            
            batch_inputs.append(input_df)
            batch_x_timestamps.append(pd.to_datetime(input_df["date"]))
            batch_y_timestamps.append(pd.to_datetime(future_df["date"]))
            batch_names.append(name)
            batch_current_closes.append(current_close)
            batch_current_dates.append(current_date)
            batch_future_dfs.append(future_df)
        
        if not batch_names:
            continue
        
        # === æ ¸å¿ƒæ”¹è¿›ï¼šæ ¹æ®å½“å‰æ—¥æœŸå¼ºåˆ¶é‡ç½®ç§å­ ===
        # ä½¿ç”¨æ—¥æœŸçš„ unix æ—¶é—´æˆ³ä½œä¸ºç§å­ï¼Œç¡®ä¿æ— è®ºå“ªå¼ å¡å¤„ç†è¿™ä¸€å¤©ï¼Œéšæœºåºåˆ—å®Œå…¨ä¸€è‡´
        date_seed = int(pd.to_datetime(current_date).timestamp())
        torch.manual_seed(date_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(date_seed)

        # ç§»é™¤ try-exceptï¼Œç¡®ä¿é”™è¯¯èƒ½è¢«ç«‹å³æŠ›å‡ºè€Œä¸æ˜¯è¢«é™é»˜å¿½ç•¥
        pred_outs = predictor.predict_batch(
            batch_inputs,
            batch_x_timestamps,
            batch_y_timestamps,
            pred_len=config['pred_len'],
            T=config['T'],
            top_p=config['top_p'],
            sample_count=config['sample_count'],
            verbose=False
        )

        for j, name in enumerate(batch_names):
            pred_out = pred_outs[j]
            row = {
                "date": batch_current_dates[j],
                "current_close": batch_current_closes[j],
            }

            for k in range(config['pred_len']):
                row[f"pred_t+{k+1}"] = pred_out.iloc[k]["close"]
                row[f"real_t+{k+1}"] = batch_future_dfs[j].iloc[k]["close"]

            local_results[name].append(row)

        if (i + 1) % log_interval == 0 or (i + 1) == len(my_dates):
            elapsed = time.time() - start_time
            print(f"   ğŸš€ [GPU-{rank}] è¿›åº¦ {i+1}/{len(my_dates)} | â±ï¸ {elapsed:.1f}s")

    if world_size > 1:
        dist.barrier()
        all_results_list = [None for _ in range(world_size)]
        dist.all_gather_object(all_results_list, local_results)

        if rank == 0:
            merged_results = {name: [] for name in indices_data.keys()}
            for gpu_results in all_results_list:
                for name in indices_data.keys():
                    merged_results[name].extend(gpu_results[name])
            local_results = merged_results

    final_results = {}
    if rank == 0:
        print("\nğŸ“Š æ±‡æ€»ç»“æœ...")
        for name in indices_data.keys():
            res_df = pd.DataFrame(local_results[name]).sort_values("date").reset_index(drop=True)
            res_df.to_csv(os.path.join(output_dir, f"predictions_{model_name}_{name}.csv"), index=False)
            final_results[name] = res_df

    return final_results
