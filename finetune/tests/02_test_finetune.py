import os
import torch
from testutils.test_utils import (
    setup_environment,
    aggregate_and_save_metrics,
    plot_all_results,
    parse_test_args,
    init_distributed_mode,
    run_distributed_inference,
)
from testutils.common_config import FINETUNE_CONFIG, INDICES, BASE_OUTPUT_DIR
from testutils.data_utils import read_test_data

setup_environment()
from model import Kronos, KronosTokenizer, KronosPredictor

rank, local_rank, world_size = init_distributed_mode()

CONFIG = FINETUNE_CONFIG | { 
    "device": torch.device(f"cuda:{local_rank}") if torch.cuda.is_available() else "cpu"
}

OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, "finetuned_test")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ================= Main Logic =================

def run_inference(combine_plots=True):
    """
    è¿è¡Œå¾®è°ƒæ¨¡å‹æµ‹è¯•
    
    Args:
        combine_plots: bool, True=æ‹¼æˆå¤§å›¾ï¼ŒFalse=ç‹¬ç«‹è¾“å‡ºæ¯ä¸ªæŒ‡æ•°å›¾è¡¨
    """
    # 0. ä¸€æ¬¡æ€§è¯»å–CSVæ–‡ä»¶
    all_data = read_test_data()
    
    # 1. åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ (Safetensors)
    if rank == 0:
        print(f"ğŸš€ Loading Finetuned Kronos from {CONFIG['model_path']}...")
    
    all_results = {}

    # ç¼“å­˜ï¼ˆæŒ‰è·¯å¾„å¤ç”¨ï¼‰
    tokenizer_cache = {}
    model_cache = {}
    
    for index in INDICES:
        # è·å–æ¯ä¸ªæŒ‡æ•°çš„æ¨¡å‹è·¯å¾„
        if isinstance(CONFIG["model_path"], dict):
            model_path = CONFIG["model_path"].get(
                index,
                CONFIG["model_path"].get(
                    "default", list(CONFIG["model_path"].values())[0]
                ),
            )
        else:
            model_path = CONFIG["model_path"]

        # è·å–æ¯ä¸ªæŒ‡æ•°å¯¹åº”çš„ tokenizer è·¯å¾„
        if isinstance(CONFIG["tokenizer_path"], dict):
            tokenizer_path = CONFIG["tokenizer_path"].get(
                index,
                CONFIG["tokenizer_path"].get(
                    "default", list(CONFIG["tokenizer_path"].values())[0]
                ),
            )
        else:
            tokenizer_path = CONFIG["tokenizer_path"]

        if tokenizer_path not in tokenizer_cache:
            tokenizer_cache[tokenizer_path] = KronosTokenizer.from_pretrained(
                tokenizer_path
            )
        tokenizer = tokenizer_cache[tokenizer_path]

        if model_path not in model_cache:
            model_cache[model_path] = Kronos.from_pretrained(model_path)
        model = model_cache[model_path]
        
        # åˆå§‹åŒ–é¢„æµ‹å™¨
        predictor = KronosPredictor(
            model, tokenizer, 
            device=CONFIG['device'], 
            max_context=512,
            clip=CONFIG['clip_val']
        )
        
        results = run_distributed_inference(
            predictor=predictor,
            all_data=all_data,
            indices_dict={index: INDICES[index]},  # åªä¼ é€’å½“å‰æŒ‡æ•°
            config=CONFIG,
            output_dir=OUTPUT_DIR,
            model_name=index,
            rank=rank,
            world_size=world_size,
        )
        all_results[index] = results  # ä¿å­˜æ¯ä¸ªæŒ‡æ•°çš„ç»“æœ

    if rank == 0:
        for index, results in all_results.items():
            aggregate_and_save_metrics(results, OUTPUT_DIR, index)
            plot_all_results(results, OUTPUT_DIR, index, CONFIG, combine_plots)

if __name__ == "__main__":
    args = parse_test_args('Test Kronos Finetuned Model')
    run_inference(combine_plots=not args.separate_plots)