import os
import glob
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import warnings

# ç¦ç”¨è­¦å‘Š
warnings.filterwarnings('ignore')
sys.path.append("/gemini/code/")

# ================= 0. å®éªŒé…ç½® (Report Standard) =================
class Config:
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    tokenizer_path = "NeoQuasar/Kronos-Tokenizer-base"
    model_path = "NeoQuasar/Kronos-base"
    
    train_dir = "/gemini/code/dataset/train"
    target_index_file = "/gemini/code/dataset/inference/000852.SH.csv"
    results_dir = "/gemini/code/results_report_final"
    
    # ç¼“å­˜è®¾å®š
    cache_file = "/gemini/code/dataset/train_cache_cs_final.pt"
    map_file = "/gemini/code/dataset/vocab_map_cs_final.pt"
    model_save_file = "/gemini/code/dataset/kronos_tuned_final.pth" # ğŸ”¥ æ•‘å‘½å­˜æ¡£æ–‡ä»¶
    
    lookback = 96
    pred_len = 5
    epochs = 3
    batch_size = 32
    lr = 5e-5
    
    top_k = 50
    rebalance_days = 5
    seed = 42

def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

from model import Kronos, KronosTokenizer

# ================= 1. ç‰©ç†ç»“æ„æ‰‹æœ¯ (ä¿æŒä¸å˜) =================
def resize_kronos_internal(model, target_vocab_size):
    print(f"ğŸ”§ [Surgery] ç‰©ç†åŒæ­¥ Kronos ç»“æ„: Vocab -> {target_vocab_size}")
    hidden_dim = 832 
    model.embedding.emb_s1 = nn.Embedding(target_vocab_size, hidden_dim).to(Config.device)
    model.embedding.emb_s2 = nn.Embedding(target_vocab_size, hidden_dim).to(Config.device)
    model.head.proj_s1 = nn.Linear(hidden_dim, target_vocab_size).to(Config.device)
    model.head.proj_s2 = nn.Linear(hidden_dim, target_vocab_size).to(Config.device)
    model.s1_vocab_size = target_vocab_size
    model.s2_vocab_size = target_vocab_size
    return model

# ================= 2. æ•°æ®å¤„ç† (ä¿æŒä¸å˜) =================
def get_data(tokenizer):
    if os.path.exists(Config.cache_file):
        print("ğŸ“¦ [Cache] åŠ è½½æ•°æ®ç¼“å­˜...")
        return torch.load(Config.cache_file), torch.load(Config.map_file)
    
    print("âš™ï¸ [Processing] å¼€å§‹å…¨é‡æ•°æ®å¤„ç†...")
    tokenizer.to(Config.device)
    files = [f for f in glob.glob(os.path.join(Config.train_dir, "*.csv")) if "000852" not in f]
    
    all_tokens = []
    batch_buffer = []
    for f in tqdm(files, desc="Parsing CSVs"):
        try:
            df = pd.read_csv(f).dropna(subset=['close'])
            if len(df) < 110: continue
            raw = df['close'].values.astype(np.float64)
            df['close'] = np.cumprod(1 + np.insert(np.diff(raw)/raw[:-1], 0, 0))
            arr = df[['open', 'high', 'low', 'close', 'volume', 'amount']].values.astype(np.float32)
            
            for start in range(0, len(arr) - 101, 5):
                seq = arr[start : start + 101]
                batch_buffer.append((seq - np.mean(seq, axis=0)) / (np.std(seq, axis=0) + 1e-6))
                
                if len(batch_buffer) >= 512:
                    with torch.no_grad():
                        t = tokenizer.encode(torch.tensor(np.array(batch_buffer)).to(Config.device))
                    if t.dim() == 2: t = t.unsqueeze(-1).repeat(1, 1, 2)
                    all_tokens.append(t.cpu())
                    batch_buffer = []
        except: continue
    
    if batch_buffer:
        with torch.no_grad():
            t = tokenizer.encode(torch.tensor(np.array(batch_buffer)).to(Config.device))
        if t.dim() == 2: t = t.unsqueeze(-1).repeat(1, 1, 2)
        all_tokens.append(t.cpu())

    full_raw = torch.cat(all_tokens, dim=0)
    rev_map = torch.unique(full_raw).sort()[0]
    mapped = torch.tensor(np.searchsorted(rev_map.numpy(), full_raw.numpy())).reshape(full_raw.shape).long()
    
    torch.save(mapped, Config.cache_file)
    torch.save(rev_map, Config.map_file)
    return mapped, rev_map

# ================= 3. å¾®è°ƒ (å¸¦è‡ªåŠ¨å­˜æ¡£åŠŸèƒ½) =================
def run_train(model, tokenizer):
    data, rmap = get_data(tokenizer)
    model = resize_kronos_internal(model, len(rmap))
    
    # ğŸ”¥ æ ¸å¿ƒä¿®æ­£ï¼šå¦‚æœå­˜åœ¨æƒé‡å­˜æ¡£ï¼Œç›´æ¥åŠ è½½ï¼Œä¸å†è®­ç»ƒï¼
    if os.path.exists(Config.model_save_file):
        print(f"ğŸ’¾ [Checkpoint] æ£€æµ‹åˆ°å·²è®­ç»ƒæ¨¡å‹: {Config.model_save_file}")
        print("â© è·³è¿‡è®­ç»ƒæ­¥éª¤ï¼Œç›´æ¥åŠ è½½æƒé‡...")
        model.load_state_dict(torch.load(Config.model_save_file))
        return model, rmap
    
    print("ğŸš€ [Training] æœªæ£€æµ‹åˆ°å­˜æ¡£ï¼Œå¼€å§‹ 4.5h è®­ç»ƒ...")
    loader = DataLoader(data, batch_size=Config.batch_size, shuffle=True)
    opt = AdamW(model.parameters(), lr=Config.lr)
    loss_fn = nn.CrossEntropyLoss()
    
    model.train()
    for e in range(Config.epochs):
        pbar = tqdm(loader, desc=f"Epoch {e+1}")
        for b in pbar:
            b = b.to(Config.device)
            out = model(b[:, :-1, 0], s2_ids=b[:, :-1, 1])
            logits = out[0] if isinstance(out, tuple) else (out.logits if hasattr(out, 'logits') else out)
            loss = loss_fn(logits[:, -5:, :].reshape(-1, len(rmap)), b[:, -5:, 0].reshape(-1))
            loss.backward(); opt.step(); opt.zero_grad()
            pbar.set_postfix(loss=f"{loss.item():.4f}")
            
    # ğŸ”¥ è®­ç»ƒå®Œç«‹åˆ»ä¿å­˜ï¼Œé˜²æ­¢ç™½è·‘
    print(f"ğŸ’¾ [Save] è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨ä¿å­˜æ¨¡å‹è‡³ {Config.model_save_file}...")
    torch.save(model.state_dict(), Config.model_save_file)
    return model, rmap

# ================= 4. æˆªé¢å›æµ‹ (ä¿®æ­£æ—¥æœŸé€»è¾‘) =================
def run_backtest(model, tokenizer, rmap):
    print("\nğŸš€ [Alpha] å¯åŠ¨æˆªé¢é€‰è‚¡å›æµ‹ (ä¿®æ­£ç‰ˆ)...")
    model.eval(); rmap = rmap.to(Config.device)
    
    idx_df = pd.read_csv(Config.target_index_file)
    idx_df['timestamp'] = pd.to_datetime(idx_df['timestamp']).dt.normalize()
    idx_df = idx_df.set_index('timestamp').sort_index()
    
    stocks = {}
    files = glob.glob(os.path.join(Config.train_dir, "*.csv"))
    # åŠ è½½å‰ 1500 ä¸ªä»¥æµ‹è¯• (æˆ–è€…å…¨é‡)
    for f in tqdm(files, desc="Loading Pool"):
        try:
            df = pd.read_csv(f)
            df['timestamp'] = pd.to_datetime(df['timestamp']).dt.normalize()
            stocks[os.path.basename(f).split('.')[0]] = df.set_index('timestamp').sort_index()
        except: continue

    # --- ğŸ”¥ å…³é”®é€»è¾‘ä¿®æ­£ï¼šä»¥æŒ‡æ•°æ—¶é—´ä¸ºå‡† (Master Clock) ---
    # ä¸è¦å»æ±‚æ‰€æœ‰è‚¡ç¥¨çš„äº¤é›†ï¼Œé‚£ä¼šå¾—åˆ°ç©ºé›†ã€‚
    # æˆ‘ä»¬åªå…³å¿ƒï¼šåœ¨æŒ‡æ•°å­˜åœ¨çš„é‚£äº›æ—¥å­é‡Œï¼Œæœ‰å“ªäº›è‚¡ç¥¨æ˜¯æ´»ç€çš„ã€‚
    
    # å–æŒ‡æ•°æœ€å 500 ä¸ªäº¤æ˜“æ—¥ä½œä¸ºæµ‹è¯•åŒºé—´ (çº¦2å¹´)
    trading_dates = idx_df.index[-500:]
    # æ¯5å¤©è°ƒä»“
    rebalance_dates = trading_dates[::5]
    
    print(f"ğŸ“… å›æµ‹åŒºé—´: {rebalance_dates[0].date()} è‡³ {rebalance_dates[-1].date()}")
    print(f"ğŸ“Š è°ƒä»“æ¬¡æ•°: {len(rebalance_dates)}")
    
    results = []
    
    for d in tqdm(rebalance_dates[:-1], desc="Rebalancing"):
        batch_x, codes = [], []
        
        # éå†æ‰€æœ‰è‚¡ç¥¨ï¼Œæ£€æŸ¥å½“å¤© d æ˜¯å¦æœ‰æ•°æ®
        for c, df in stocks.items():
            if d not in df.index: continue # å½“å¤©åœç‰Œæˆ–æœªä¸Šå¸‚/å·²é€€å¸‚ï¼Œè·³è¿‡
            
            pos = df.index.get_loc(d)
            if pos < 96: continue # ä¸Šå¸‚æ—¶é—´å¤ªçŸ­ï¼Œä¸å¤Ÿ lookback
            
            # æˆªå–çª—å£
            win = df.iloc[pos-95 : pos+1][['open', 'high', 'low', 'close', 'volume', 'amount']].values.astype(np.float32)
            # ç®€å•çš„ Z-Score
            batch_x.append((win - np.mean(win, axis=0)) / (np.std(win, axis=0) + 1e-6))
            codes.append(c)
        
        # å¦‚æœå½“å¤©ä¸€åªè‚¡ç¥¨éƒ½æ²¡æœ‰ (åŸºæœ¬ä¸å¯èƒ½ï¼Œé™¤éæ•°æ®æºå…¨æ˜¯é”™çš„)
        if not batch_x: 
            # print(f"âš ï¸ {d.date()} æ— è‚¡ç¥¨æ•°æ®")
            continue
            
        with torch.no_grad():
            tensor_in = torch.tensor(np.array(batch_x)).to(Config.device)
            # Tokenize
            toks = tokenizer.encode(tensor_in)
            if toks.dim() == 2: toks = toks.unsqueeze(-1).repeat(1, 1, 2)
            
            # Align
            dense = torch.searchsorted(rmap, torch.clamp(toks, rmap[0], rmap[-1])).reshape(toks.shape)
            dense = torch.clamp(dense, 0, len(rmap)-1)
            
            # Predict
            out = model(dense[:, :, 0], s2_ids=dense[:, :, 1])
            logits = out[0] if isinstance(out, tuple) else (out.logits if hasattr(out, 'logits') else out)
            
            # Score: å– Close (idx 3) çš„é¢„æµ‹å€¼
            score = tokenizer.decode(rmap[torch.argmax(logits[:, -1, :], dim=-1)])[:, 3].cpu().numpy()

        # é€‰è‚¡
        rank = pd.DataFrame({'c': codes, 's': score}).sort_values('s', ascending=False)
        top = rank.head(50)['c'].tolist()
        
        # è®¡ç®—æœªæ¥ 5 å¤©æ”¶ç›Š
        rets = []
        for c in top:
            s_df = stocks[c]
            try:
                p = s_df.index.get_loc(d)
                # ä¹°å…¥ä»·: d çš„æ”¶ç›˜ä»· (ç®€åŒ–å¤„ç†)
                # å–å‡ºä»·: d+5 çš„æ”¶ç›˜ä»·
                idx_sell = min(p+5, len(s_df)-1)
                r = s_df['close'].iloc[idx_sell] / s_df['close'].iloc[p] - 1
                rets.append(r)
            except: pass
        
        # è®¡ç®—æŒ‡æ•°åŒæœŸæ”¶ç›Š
        try:
            b_p = idx_df.index.get_loc(d)
            b_idx_sell = min(b_p+5, len(idx_df)-1)
            b_ret = idx_df['close'].iloc[b_idx_sell] / idx_df['close'].iloc[b_p] - 1
        except: b_ret = 0.0
        
        results.append({'d': d, 'r': np.mean(rets) if rets else 0.0, 'b': b_ret})
        
        # æ¸…ç†æ˜¾å­˜ï¼Œé˜²æ­¢ OOM
        torch.cuda.empty_cache()

    if results:
        res = pd.DataFrame(results).set_index('d')
        res['ex'] = res['r'] - res['b']
        res['nav'] = (1 + res['ex']).cumprod()
        res['bench'] = (1 + res['b']).cumprod()
        
        ir = (res['ex'].mean() / (res['ex'].std() + 1e-9)) * np.sqrt(252/5)
        
        plt.figure(figsize=(10, 5))
        plt.plot(res.index, res['nav'], label=f'Alpha (IR: {ir:.2f})', color='red')
        plt.plot(res.index, res['bench'], label='Benchmark', color='grey', alpha=0.5)
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(Config.results_dir, "alpha_final.png"))
        print(f"\nâœ… è¿è¡Œå®Œæˆ | IR: {ir:.4f} | ç´¯è®¡è¶…é¢: {res['nav'].iloc[-1]:.4f}")
    else:
        print("\nâŒ ä¾ç„¶æ²¡æœ‰ç»“æœã€‚è¯·æ£€æŸ¥æŒ‡æ•° CSV å’Œä¸ªè‚¡ CSV çš„æ—¥æœŸæ ¼å¼æ˜¯å¦å®Œå…¨ä¸€è‡´ (YYYY-MM-DD)ã€‚")

if __name__ == "__main__":
    set_seed(Config.seed)
    tk = KronosTokenizer.from_pretrained(Config.tokenizer_path)
    md = Kronos.from_pretrained(Config.model_path).to(Config.device)
    md, rm = run_train(md, tk)
    run_backtest(md, tk, rm)